{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Single Feature Importance with ICECREAM: Cloud Experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment (see also Section 5.2 of the paper), we apply ICECREAM to the Cloud Computing Application defined in the paper and appendix.\n",
    "\n",
    "The notebook is meant to enable the user to re-produce our results. It is split into four sections:\n",
    "\n",
    "- Setup: This section imports the required modules and defines a few utility functions.\n",
    "\n",
    "- Preparation: This section generates a synthetic dataset from a `generating_causal_network` with given parameters. It then calculates and stores the ground truth, and fits a new causal network to the generated samples, such that only the structure of the network needs to be known. The generated data is stored to a folder (the default is `/data`). The `/original_data` folder already contains the samples used for the results in the paper, so this section can be skipped.\n",
    "\n",
    "- Calculation of scores: This is the calculation of minimal full-explanation coalitions with ICECREAM, and of anomaly scores using the baseline methods. \n",
    "  \n",
    "  **WARNING: For large numbers of samples (e.g., the 10_000_000 samples that were used in the paper), this can take a very long time!**\n",
    "\n",
    "- Analysis: This section loads the data from the previous sections and performs the same analysis we are showing in the paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dowhy.gcm.anomaly import attribute_anomalies\n",
    "\n",
    "from explain import CausalNetwork, expected_value, noisify\n",
    "from explain.explanations.causal_explanation import exact_explanation_score, find_minimum_size_coalitions\n",
    "from causal_models import CloudServiceErrorModel, CloudServiceErrorRootModel, CloudServiceErrorConditionalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsets(s, include_empty_set=True):\n",
    "    return map(set, itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(0 if include_empty_set else 1, len(s) + 1)))\n",
    "\n",
    "def find_ground_truth_minimum_size_coalitions(causal_network, sample, target, ground_truth_error_nodes, *, threshold=0.99999):\n",
    "    coalitions = [coalition for coalition in subsets(ground_truth_error_nodes, include_empty_set=False) if expected_value(exact_explanation_score(causal_network, pd.Series(sample), target, coalition)) >= threshold]\n",
    "    min_size = min(map(len, coalitions))\n",
    "    return [coalition for coalition in coalitions if len(coalition) == min_size]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a causal network for generating the ground truth. From this network, we draw samples, separate the error samples and calculate the ground truth. All data generated in this section is stored to a folder such that it can be loaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folder for data storage and loading (pre-calculated data is stored at `/original`)\n",
    "folder = 'data'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generating causal network\n",
    "\n",
    "order_db = CloudServiceErrorRootModel(p=0.05) # X_7\n",
    "customer_db = CloudServiceErrorRootModel(p=0.01) # X_4\n",
    "shipping = CloudServiceErrorRootModel(p=0.03) # X_6\n",
    "product_db = CloudServiceErrorRootModel(p=0.02) # X_1\n",
    "order = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel], t=1, p=0.01) # X_8\n",
    "auth = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel], t=1, p=0.02) # X_5\n",
    "product = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel] * 3, t=2, p=0.01) # X_3\n",
    "caching = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel], t=1, p=0.01) # X_2\n",
    "api = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel] * 4, t=3, p=0.01) # X_9\n",
    "www = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel] * 2, t=2, p=0.0) # Y\n",
    "\n",
    "generating_causal_network = CausalNetwork(\n",
    "    {'order_db': order_db, 'customer_db': customer_db, 'shipping': shipping, 'product_db': product_db,\n",
    "        'order': (order, ['order_db']), 'auth': (auth, ['customer_db']),\n",
    "        'product': (product, ['customer_db', 'shipping', 'caching']), 'caching': (caching, ['product_db']),\n",
    "        'api': (api, ['customer_db', 'order', 'auth', 'product']), 'www': (www, ['auth', 'api'])\n",
    "    }\n",
    ")\n",
    "\n",
    "nodes = list(generating_causal_network.nodes)\n",
    "noise = noisify(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create samples\n",
    "\n",
    "number_of_samples = 10_000_000\n",
    "noise_samples = generating_causal_network.draw_noise_samples(number_of_samples)[noise]\n",
    "\n",
    "samples_number_of_errors = noise_samples.astype(int).sum(axis=1)\n",
    "samples_number_of_errors.groupby(samples_number_of_errors).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store compressed samples and error samples to save space\n",
    "print(f'Storing compressed samples...')\n",
    "ground_truth_compressed = list(noise_samples.progress_apply(lambda row: sum(int(value) * 2 ** index for index, value in enumerate(reversed(row))), axis=1, result_type='reduce'))\n",
    "\n",
    "with open(f'{folder}/compressed.pkl', 'wb') as f:\n",
    "    pickle.dump(ground_truth_compressed, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Recovering full samples...')\n",
    "samples = generating_causal_network.from_noise(noise_samples).astype(str)\n",
    "\n",
    "print(f'Calculating error samples...')\n",
    "error_samples = samples[samples['www'] == '1'].copy()\n",
    "\n",
    "error_samples['sample'] = error_samples[nodes].to_dict('records')\n",
    "error_samples['error_nodes'] = error_samples.progress_apply(lambda row: {node for node in generating_causal_network.nodes if row[f'_{node}'] == '1'}, axis=1)\n",
    "error_samples['minimal_coalitions'] = error_samples.progress_apply(lambda row: find_ground_truth_minimum_size_coalitions(generating_causal_network, row['sample'], 'www', row['error_nodes']), axis=1, result_type='reduce')\n",
    "\n",
    "print(f'Storing error samples...')\n",
    "error_samples.to_pickle(f'{folder}/error_samples.pkl', protocol=4)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover full samples using original causal network\n",
    "print('Reading compressed samples...')\n",
    "ground_truth_compressed = pd.read_pickle(f'{folder}/compressed.pkl')\n",
    "print('Decompressing samples...')\n",
    "ground_truth = [[(number >> 9 - n) & 1 for n in range(10)] for number in ground_truth_compressed]\n",
    "\n",
    "print('Recovering full samples...')\n",
    "noise_samples = pd.DataFrame(ground_truth, columns=noise)\n",
    "samples = generating_causal_network.from_noise(noise_samples).astype(str)\n",
    "\n",
    "# Create causal network with dummy parameters and fit to samples\n",
    "print('Fitting causal network...')\n",
    "order_db = CloudServiceErrorRootModel(p=0.0)\n",
    "customer_db = CloudServiceErrorRootModel(p=0.0)\n",
    "shipping = CloudServiceErrorRootModel(p=0.0)\n",
    "product_db = CloudServiceErrorRootModel(p=0.0)\n",
    "order = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel], t=0, p=0.0)\n",
    "auth = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel], t=0, p=0.0)\n",
    "product = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel] * 3, t=0, p=0.0)\n",
    "caching = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel], t=0, p=0.0)\n",
    "api = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel] * 4, t=0, p=0.0)\n",
    "www = CloudServiceErrorConditionalModel(parent_signature=[CloudServiceErrorModel] * 2, t=0, p=0.0)\n",
    "\n",
    "causal_network = CausalNetwork(\n",
    "    {'order_db': order_db, 'customer_db': customer_db, 'shipping': shipping, 'product_db': product_db,\n",
    "        'order': (order, ['order_db']), 'auth': (auth, ['customer_db']),\n",
    "        'product': (product, ['customer_db', 'shipping', 'caching']), 'caching': (caching, ['product_db']),\n",
    "        'api': (api, ['customer_db', 'order', 'auth', 'product']), 'www': (www, ['auth', 'api'])\n",
    "    }\n",
    ")\n",
    "\n",
    "causal_network.fit(samples)\n",
    "\n",
    "with open(f'{folder}/causal_network.pkl', 'wb') as f:\n",
    "    pickle.dump(causal_network, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the fitted causal network and the error samples, and calculate the scores (explanation score and anomaly scores) using ICECREAM and the baseline methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to manually set folder\n",
    "# folder = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store scores\n",
    "\n",
    "explanation_score_threshold = 0.9998\n",
    "num_rca_distribution_samples = 1_000\n",
    "\n",
    "with open(f'{folder}/causal_network.pkl', 'rb') as f:\n",
    "    causal_network = pickle.load(f)\n",
    "    nodes = list(causal_network.nodes)\n",
    "    noise = noisify(nodes)\n",
    "\n",
    "# Only load observation columns from error samples, not the ground truth\n",
    "with open(f'{folder}/error_samples.pkl', 'rb') as f:\n",
    "    error_samples = pickle.load(f)[nodes]\n",
    "\n",
    "print(f'Calculating explanation scores...')\n",
    "explanation_score_result = [find_minimum_size_coalitions(causal_network, sample, \"www\", threshold=explanation_score_threshold) for _, sample in tqdm(error_samples.iterrows())]\n",
    "with open(f'{folder}/explanation_score.pkl', 'wb') as f:\n",
    "    pickle.dump(explanation_score_result, f, protocol=4)\n",
    "\n",
    "print(f'Calculating IT RCA scores...')\n",
    "anomaly_scores = pd.DataFrame(attribute_anomalies(causal_network, 'www', error_samples, attribute_mean_deviation=False, num_distribution_samples=num_rca_distribution_samples))\n",
    "with open(f'{folder}/outlier_rca.pkl', 'wb') as f:\n",
    "    pickle.dump(anomaly_scores, f, protocol=4)\n",
    "\n",
    "print(f'Calculating mean deviation RCA scores...')\n",
    "mean_deviation_scores = pd.DataFrame(attribute_anomalies(causal_network, 'www', error_samples, attribute_mean_deviation=True, num_distribution_samples=num_rca_distribution_samples))\n",
    "with open(f'{folder}/mean_deviation_rca.pkl', 'wb') as f:\n",
    "    pickle.dump(mean_deviation_scores, f, protocol=4)\n",
    "\n",
    "print(f'Calculating traversal RCA scores...')\n",
    "def anomaly_traversal(causal_graph, anomaly_nodes):\n",
    "    return {node for node in anomaly_nodes if not set(anomaly_nodes) & set(causal_graph.predecessors(node))}\n",
    "\n",
    "traversal_rca_nodes = list(error_samples[nodes].apply(lambda x: x == '1').apply(lambda x: list(error_samples.columns[x.values]), axis=1))\n",
    "traversal_rca_result = [anomaly_traversal(causal_network.graph, error_nodes) for error_nodes in traversal_rca_nodes]\n",
    "with open(f'{folder}/traversal_rca.pkl', 'wb') as f:\n",
    "    pickle.dump(traversal_rca_result, f, protocol=4)\n",
    "\n",
    "print(f'Calculating alternative traversal RCA scores...')\n",
    "def anomaly_traversal_2(causal_graph, target, anomaly_nodes):\n",
    "    nodes, anormal_nodes = {target}, set()\n",
    "\n",
    "    while nodes:\n",
    "        node = nodes.pop()\n",
    "        if node in anomaly_nodes:\n",
    "            if not (anomaly_nodes & set(causal_graph.predecessors(node))):\n",
    "                anormal_nodes.add(node)\n",
    "            else:\n",
    "                nodes.update(anomaly_nodes & set(causal_graph.predecessors(node)))\n",
    "\n",
    "    return anormal_nodes\n",
    "\n",
    "traversal_rca_2_result = [anomaly_traversal_2(causal_network.graph, 'www', set(error_nodes)) for error_nodes in traversal_rca_nodes]\n",
    "with open(f'{folder}/traversal_rca_2.pkl', 'wb') as f:\n",
    "    pickle.dump(traversal_rca_2_result, f, protocol=4)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to manually set folder\n",
    "# folder = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{folder}/causal_network.pkl', 'rb') as f:\n",
    "    causal_network = pickle.load(f)\n",
    "    nodes = list(causal_network.nodes)\n",
    "    noise = noisify(nodes)\n",
    "\n",
    "# Load compressed data, decode into dataframe and load error samples\n",
    "print(f'Loading compressed samples...')\n",
    "ground_truth_compressed = pd.read_pickle(f'{folder}/compressed.pkl')\n",
    "ground_truth = [[(number >> 9 - n) & 1 for n in range(10)] for number in tqdm(ground_truth_compressed)]\n",
    "\n",
    "print(f'Recovering full samples...')\n",
    "noise_samples = pd.DataFrame(ground_truth, columns=noise)\n",
    "samples = generating_causal_network.from_noise(noise_samples).astype(str)\n",
    "\n",
    "normal_samples = samples[samples['www'] == '0'].copy()\n",
    "\n",
    "# This time, load full error samples (including ground truth) for analysis\n",
    "print('Loading error samples...')\n",
    "with open(f'{folder}/error_samples.pkl', 'rb') as f:\n",
    "    error_samples = pickle.load(f)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table with number of samples by number of errors\n",
    "normal_samples_number_of_errors = normal_samples[noise].astype(int).sum(axis=1)\n",
    "error_samples_number_of_errors = error_samples[noise].astype(int).sum(axis=1)\n",
    "\n",
    "table = pd.concat([normal_samples_number_of_errors.groupby(normal_samples_number_of_errors).count(), error_samples_number_of_errors.groupby(error_samples_number_of_errors).count()], axis=1).fillna(0).astype(int)\n",
    "table.columns = ['Y=0', 'Y=1']\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw results and create result dataframe\n",
    "\n",
    "outlier_rca_absolute_threshold = 0.15\n",
    "outlier_rca_cumulative_threshold = 0.95\n",
    "mean_deviation_rca_absolute_threshold = 0.15\n",
    "mean_deviation_rca_cumulative_threshold = 0.95\n",
    "\n",
    "with open(f'{folder}/error_samples.pkl', 'rb') as f:\n",
    "    error_samples = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}/causal_network.pkl', 'rb') as f:\n",
    "    causal_network = pickle.load(f)\n",
    "    nodes = list(causal_network.nodes)\n",
    "    noise = noisify(nodes)\n",
    "\n",
    "with open(f'{folder}/explanation_score.pkl', 'rb') as f:\n",
    "    explanation_score_result = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}/outlier_rca.pkl', 'rb') as f:\n",
    "    anomaly_scores = pickle.load(f)\n",
    "\n",
    "outlier_rca_result = list(anomaly_scores.apply(lambda row: set(anomaly_scores.columns[row >= outlier_rca_absolute_threshold]) & (set(nodes)), axis=1))\n",
    "\n",
    "outlier_rca_result_2 = []\n",
    "for index, row in anomaly_scores.iterrows():\n",
    "    scores = row.sort_values(ascending=False).cumsum()\n",
    "    outlier_rca_result_2.append(set(scores.index[:(scores >= outlier_rca_cumulative_threshold * row.sum()).argmax() + 1]))\n",
    "\n",
    "with open(f'{folder}/mean_deviation_rca.pkl', 'rb') as f:\n",
    "    mean_deviation_scores = pickle.load(f)\n",
    "\n",
    "mean_rca_result = list(mean_deviation_scores.apply(\n",
    "    lambda row: set(mean_deviation_scores.columns[row >= mean_deviation_rca_absolute_threshold]) & (set(causal_network.nodes)),\n",
    "    axis=1))\n",
    "\n",
    "mean_rca_result_2 = []\n",
    "for index, row in mean_deviation_scores.iterrows():\n",
    "    scores = row.sort_values(ascending=False).cumsum()\n",
    "    mean_rca_result_2.append(set(scores.index[:(scores >= mean_deviation_rca_cumulative_threshold * row.sum()).argmax() + 1]))\n",
    "\n",
    "with open(f'{folder}/traversal_rca.pkl', 'rb') as f:\n",
    "    traversal_rca_result = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}/traversal_rca_2.pkl', 'rb') as f:\n",
    "    traversal_rca_result_2 = pickle.load(f)\n",
    "\n",
    "result = error_samples.copy()\n",
    "result['num_errors'] = result['error_nodes'].apply(len)\n",
    "result['error_nodes_union'] = result['minimal_coalitions'].apply(lambda minimal_coalitions: set().union(*minimal_coalitions))\n",
    "\n",
    "result['explanation_score_minimal_coalitions'] = explanation_score_result\n",
    "result['explanation_score_error_nodes'] = result['explanation_score_minimal_coalitions'].apply(lambda minimal_coalitions: set().union(*minimal_coalitions))\n",
    "result['outlier_rca_error_nodes'] = outlier_rca_result\n",
    "result['outlier_rca_2_error_nodes'] = outlier_rca_result_2\n",
    "result['mean_rca_error_nodes'] = mean_rca_result\n",
    "result['mean_rca_2_error_nodes'] = mean_rca_result_2\n",
    "result['traversal_rca_error_nodes'] = traversal_rca_result\n",
    "result['traversal_rca_2_error_nodes'] = traversal_rca_result_2\n",
    "\n",
    "with open(f'{folder}/result.pkl', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to start with stored results \n",
    "# with open(f'{folder}/result.pkl', 'rb') as f:\n",
    "#     result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First metric: Accuracy\n",
    "\n",
    "accuracy = result.copy()\n",
    "\n",
    "accuracy['explanation_score_correct'] = accuracy.apply(lambda row: all(coalition in row['minimal_coalitions'] for coalition in row['explanation_score_minimal_coalitions']), axis=1)\n",
    "accuracy['outlier_rca_correct'] = accuracy.apply(\n",
    "    lambda row: row['outlier_rca_error_nodes'] in row['minimal_coalitions'], axis=1)\n",
    "accuracy['outlier_rca_2_correct'] = accuracy.apply(\n",
    "    lambda row: row['outlier_rca_2_error_nodes'] in row['minimal_coalitions'], axis=1)\n",
    "accuracy['mean_rca_correct'] = accuracy.apply(\n",
    "    lambda row: row['mean_rca_error_nodes'] in row['minimal_coalitions'], axis=1)\n",
    "accuracy['mean_rca_2_correct'] = accuracy.apply(\n",
    "    lambda row: row['mean_rca_2_error_nodes'] in row['minimal_coalitions'], axis=1)\n",
    "accuracy['traversal_rca_correct'] = accuracy.apply(\n",
    "    lambda row: row['traversal_rca_error_nodes'] in row['minimal_coalitions'], axis=1)\n",
    "accuracy['traversal_rca_2_correct'] = accuracy.apply(\n",
    "    lambda row: row['traversal_rca_2_error_nodes'] in row['minimal_coalitions'], axis=1)\n",
    "\n",
    "accuracy_columns = ['explanation_score_correct', 'outlier_rca_correct', 'outlier_rca_2_correct',\n",
    "                        'mean_rca_correct', 'mean_rca_2_correct', 'traversal_rca_correct', 'traversal_rca_2_correct']\n",
    "\n",
    "ax = accuracy.groupby('num_errors')[accuracy_columns].mean().plot(marker='o', ylabel='Accurary',\n",
    "                                                                            xlabel='Number of original errors',\n",
    "                                                                            xticks=list(accuracy.groupby(\n",
    "                                                                                'num_errors').groups.keys()))\n",
    "markers = ['s','o', 'D', 'P', '^', 'v', 'x']\n",
    "for line, marker in zip(ax.get_lines(), markers):\n",
    "    line.set_marker(marker)\n",
    "\n",
    "ax.legend(['ICECREAm (ours)', 'IT-RCA-i', 'IT-RCA-c', 'Mean-RCA-i', 'Mean-RCA-c', 'Simple Traversal RCA', 'Backtracking Traversal RCA'])\n",
    "\n",
    "plt.savefig(f'{folder}/accuracy.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second metric: Confusion (true positives, false negatives, false positives)\n",
    "\n",
    "confusion = result.loc[result['num_errors'] > 2].copy()\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(9, 6))\n",
    "\n",
    "methods = ['explanation_score', 'outlier_rca', 'outlier_rca_2', 'mean_rca', 'mean_rca_2', 'traversal_rca']\n",
    "method_names = ['ICECREAm (ours)', 'IT-RCA-i', 'IT-RCA-c', 'Mean-RCA-i', 'Mean-RCA-c', 'Traversal RCA']\n",
    "\n",
    "for ax, method, method_name in zip(axs.reshape(6), methods, method_names):\n",
    "    data = np.array([confusion.apply(lambda row: len(row['error_nodes_union'] & row[f'{method}_error_nodes']), axis=1).mean(),\n",
    "        confusion.apply(lambda row: len(row['error_nodes_union'] - row[f'{method}_error_nodes']), axis=1).mean(),\n",
    "        confusion.apply(lambda row: len(row[f'{method}_error_nodes'] - row['error_nodes_union']),axis=1).mean()\n",
    "    ])\n",
    "    ax.pie(data, colors=['#009E73', '#0072B2', '#E69F00'])\n",
    "    ax.set_title(method_name)\n",
    "\n",
    "fig.legend(['True positives', 'False negatives', 'False positives'], loc='center', bbox_to_anchor=(0.5, 0.1), ncols=3)\n",
    "plt.subplots_adjust(wspace=0)\n",
    "plt.savefig(f'{folder}/confusion-geq-3.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = result.loc[result['num_errors'] > 2].copy()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(9, 6))\n",
    "\n",
    "for ax, method, method_name in zip(axs, ['traversal_rca', 'traversal_rca_2'], ['Simple Traversal RCA', 'Backtracking Traversal RCA']):\n",
    "    data = np.array([confusion.apply(lambda row: len(row['error_nodes_union'] & row[f'{method}_error_nodes']), axis=1).mean(),\n",
    "        confusion.apply(lambda row: len(row['error_nodes_union'] - row[f'{method}_error_nodes']), axis=1).mean(),\n",
    "        confusion.apply(lambda row: len(row[f'{method}_error_nodes'] - row['error_nodes_union']),axis=1).mean()\n",
    "    ])\n",
    "    ax.pie(data, colors=['#009E73', '#0072B2', '#E69F00'])\n",
    "    ax.set_title(method_name)\n",
    "\n",
    "fig.legend(['True positives', 'False negatives', 'False positives'], loc='center', bbox_to_anchor=(0.5, 0.2), ncols=3)\n",
    "plt.subplots_adjust(wspace=0)\n",
    "plt.savefig(f'{folder}/confusion-geq-3-traversal.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
