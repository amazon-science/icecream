{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Single Feature Importance with ICECREAM: Credit Experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment (see also Section 5.1 of the paper), we apply ICECREAM to the South German Credit dataset by Gr√∂mping (2019).\n",
    "\n",
    "The notebook is meant to enable the user to re-produce our results, either by fully repeating all calculations, or by loading the results of the time-consuming tasks from the `/data` folder and only creating the graphs. Therefore, the notebook is split into four sections:\n",
    "- Setup: This section imports the required modules and initializes the `shap` package for visual output.\n",
    "\n",
    "- Preparation: This section prepares the dataset (since `shap` cannot work with string-formatted categorical features), trains the base model and computes the SHAP values. It also stores all generated data in a folder (the default is `/data`), and can therefore be skipped when executing the notebook.\n",
    "\n",
    "- Calculation of explanation scores: This is the calculation of minimal full-explanation coalitions with ICECREAM. The `/original_data` folder already contains the original results of this calculation, so it can also be skipped.\n",
    "\n",
    "- Analysis: This section loads the data from the previous sections and performs the same analysis we are showing in the paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import set_config\n",
    "import shap\n",
    "\n",
    "from explain.explanations.model_explanation import find_minimum_size_coalitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we prepare the data, train a random forest classifier and calculate the SHAP values of this classifier for all samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folder for data storage and loading (pre-calculated data is stored at `/original`)\n",
    "folder = 'data'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "credit_data = pd.read_csv(f'german_credit_data.csv')\n",
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "data = credit_data.copy()\n",
    "model_target = 'credit_risk'\n",
    "\n",
    "numerical_features = data.drop(model_target, axis=1).select_dtypes(include='int64').columns.tolist()\n",
    "categorical_features = data.drop(model_target, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "model_features = categorical_features + numerical_features\n",
    "\n",
    "# Convert categorical values to the integer codes so that SHAP can handle them\n",
    "data[categorical_features] = data[categorical_features].astype('category').apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store pre-processed credit data\n",
    "with open(f'{folder}/data.pkl', 'wb') as f:\n",
    "  pickle.dump(data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train classifier\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, shuffle=True, random_state=23)\n",
    "\n",
    "X_train = train_data.drop(model_target, axis=1)\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "X_test = test_data.drop(model_target, axis=1)\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "X_total = data.drop(model_target, axis=1)\n",
    "y_total = data[model_target]\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [\n",
    "        (\"num_imputer\", SimpleImputer(strategy=\"mean\", add_indicator=True)),\n",
    "        (\"num_scaler\", MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline(\n",
    "    [\n",
    "        (\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine all data preprocessors from above\n",
    "data_processor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_processing\", numerical_processor, numerical_features),\n",
    "        (\"categorical_processing\", categorical_processor, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "random_forest = RandomForestClassifier(max_depth=10, random_state=1)\n",
    "\n",
    "# Pipeline with desired data transformers, along with an estimator at the end\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_processing\", data_processor),\n",
    "        (\"rf\", random_forest),\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifier = pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(f'Accuracy (train / test): {accuracy_score(y_train, classifier.predict(X_train)):.2f} / {accuracy_score(y_test, classifier.predict(X_test)):.2f}')\n",
    "\n",
    "# Show pipeline\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store classifier\n",
    "with open(f'{folder}/classifier.pkl', 'wb') as f:\n",
    "  pickle.dump(classifier, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "explainer = shap.Explainer(classifier.predict, X_total)\n",
    "shap_values = explainer(X_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show SHAP waterfall for some sample\n",
    "shap.plots.waterfall(shap_values[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store SHAP values\n",
    "with open(f'{folder}/shap.pkl', 'wb') as f:\n",
    "  pickle.dump(shap_values, f, protocol=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of explanation scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply ICECREAM to the dataset, calculating minimum-size coalitions with (approximately) full explanation score and at most four elements for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9998\n",
    "k = 4\n",
    "\n",
    "coalitions = [find_minimum_size_coalitions(data, observation, 'credit_risk', classifier=classifier.predict, threshold=alpha, maximum_coalition_size=k) for _, observation in tqdm(list(data.head(50).iterrows()))]\n",
    "\n",
    "with open(f'{folder}/coalitions.pkl', 'wb') as f:\n",
    "    pickle.dump(coalitions, f, protocol=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the SHAP and ICECREAM results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, classifier, SHAP values and minimum-size coalitions from /data\n",
    "\n",
    "with open(f'{folder}/data.pkl', 'rb') as f:\n",
    "  data = pickle.load(f)\n",
    "\n",
    "model_target = 'credit_risk'\n",
    "\n",
    "X_total = data.drop(model_target, axis=1)\n",
    "y_total = data[model_target]\n",
    "\n",
    "model_features = list(X_total.columns)\n",
    "\n",
    "\n",
    "with open(f'{folder}/classifier.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}/shap.pkl', 'rb') as f:\n",
    "    shap_values = pickle.load(f)\n",
    "    shap_df = pd.DataFrame(shap_values.values, columns=model_features)\n",
    "    normalized_shap_df = shap_df.div(shap_df.sum(axis=1), axis=0)\n",
    "\n",
    "with open(f'{folder}/coalitions.pkl', 'rb') as f:\n",
    "    minimum_size_coalitions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify simple samples, i.e., samples which have at least one few-feature, full-explanation score coalition\n",
    "simple_explanation_indices = [i for i, coalitions in enumerate(minimum_size_coalitions) if len(coalitions) > 0]\n",
    "\n",
    "print(f'There are {len(simple_explanation_indices)}/{len(minimum_size_coalitions)} samples with simple (k <= 4, alpha >= 0.9998) explanations: {simple_explanation_indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all such coalitions, together with the sample they refer to\n",
    "coalitions = pd.DataFrame([(index, coalition) for index in simple_explanation_indices for coalition in minimum_size_coalitions[index]], columns=['sample_index', 'coalition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successively randomize features in three different orders and compare the prediction stability of the model\n",
    "\n",
    "def create_randomized_samples(features, number_of_samples=1_000):\n",
    "    return X_total[features].sample(n=number_of_samples, replace=True, ignore_index=True)\n",
    "\n",
    "number_of_samples = 1_000\n",
    "\n",
    "result_non_coalition_features, result_non_top_shap_features, result_random_features = [], [], []\n",
    "for index, [sample_index, coalition] in tqdm(coalitions.iterrows(), total=len(coalitions)):\n",
    "    non_top_shap_features = list(normalized_shap_df.iloc[sample_index].sort_values(ascending=True).index)\n",
    "    non_coalition_features = sorted(non_top_shap_features, key=lambda feature: feature in coalition)\n",
    "    random_features = model_features.copy()\n",
    "    random.shuffle(random_features)\n",
    "\n",
    "    congruence_non_coalition_features, congruence_non_top_shap_features, congruence_random_features = [], [], []\n",
    "    for number_of_randomized_features in range(len(model_features) + 1):\n",
    "        # Randomize in order of non_coalition_features and measure congruence\n",
    "        samples = X_total.loc[[sample_index] * number_of_samples].reset_index(drop=True)\n",
    "        samples[non_coalition_features[:number_of_randomized_features]] = create_randomized_samples(non_coalition_features[:number_of_randomized_features], number_of_samples=number_of_samples)\n",
    "\n",
    "        congruence_non_coalition_features.append((classifier.predict(samples) == y_total[sample_index]).mean())\n",
    "\n",
    "        # Randomize in order of non_top_shap_features and measure congruence\n",
    "        samples = X_total.loc[[sample_index] * number_of_samples].reset_index(drop=True)\n",
    "        samples[non_top_shap_features[:number_of_randomized_features]] = create_randomized_samples(non_top_shap_features[:number_of_randomized_features], number_of_samples=number_of_samples)\n",
    "\n",
    "        congruence_non_top_shap_features.append((classifier.predict(samples) == y_total[sample_index]).mean())\n",
    "\n",
    "        # Randomize in order of random_features and measure congruence\n",
    "        samples = X_total.loc[[sample_index] * number_of_samples].reset_index(drop=True)\n",
    "        samples[random_features[:number_of_randomized_features]] = create_randomized_samples(random_features[:number_of_randomized_features], number_of_samples=number_of_samples)\n",
    "\n",
    "        congruence_random_features.append((classifier.predict(samples) == y_total[sample_index]).mean())\n",
    "\n",
    "    result_non_top_shap_features.append(congruence_non_top_shap_features)\n",
    "    result_non_coalition_features.append(congruence_non_coalition_features)\n",
    "    result_random_features.append(congruence_random_features)\n",
    "\n",
    "y_non_top_shap_features = np.array(result_non_top_shap_features)\n",
    "y_non_coalition_features = np.array(result_non_coalition_features)\n",
    "y_random_features = np.array(result_random_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustration_sample_index = 680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "colors=['#009E73', '#E69F00', '#0072B2']\n",
    "\n",
    "# Plot illustration sample\n",
    "plt.plot(y_non_coalition_features[illustration_sample_index], '.-', label='Randomize non-coalition features first', color=colors[0])\n",
    "plt.plot(y_non_top_shap_features[illustration_sample_index], 'x-', label='Randomize in ascending order of SHAP value', color=colors[1])\n",
    "plt.plot(y_random_features[illustration_sample_index], 's-', label='Randomize in random order', color=colors[2], markersize=3)\n",
    "\n",
    "# Plot mean and error bars for all samples\n",
    "plt.plot(np.mean(y_non_coalition_features, axis=0), '--', color=colors[0])\n",
    "plt.fill_between(range(len(model_features) + 1), np.quantile(y_non_coalition_features, 0.05, axis=0), np.quantile(y_non_coalition_features, 0.95, axis=0), color=colors[0], alpha=0.2)\n",
    "plt.plot(np.mean(y_non_top_shap_features, axis=0), '--', color=colors[1])\n",
    "plt.fill_between(range(len(model_features) + 1), np.quantile(y_non_top_shap_features, 0.05, axis=0), np.quantile(y_non_top_shap_features, 0.95, axis=0), color=colors[1], alpha=0.2)\n",
    "plt.plot(np.mean(y_random_features, axis=0), '--', color=colors[2])\n",
    "plt.fill_between(range(len(model_features) + 1), np.quantile(y_random_features, 0.05, axis=0), np.quantile(y_random_features, 0.95, axis=0), color=colors[2], alpha=0.2)\n",
    "\n",
    "# Show where coalition features are randomized\n",
    "plt.annotate('Start randomizing \\n coalition features', xy=(18.2, 0.998), xytext=(20, 0.98), backgroundcolor='w', arrowprops=dict(arrowstyle='-|>', facecolor='gray', edgecolor='gray'))\n",
    "\n",
    "plt.ylabel(r'Stability $\\mathbb{P}[Y=y \\mid do(\\mathbf{V}_C = \\mathbf{v}_C)]$ of prediction')\n",
    "plt.xlabel('Number of randomized features')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'{folder}/result.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
