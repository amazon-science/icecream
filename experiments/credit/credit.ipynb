{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Single Feature Importance with ICECREAM: Credit Experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment (see also Section 5.1 of the paper), we apply ICECREAM to the South German Credit dataset by GrÃ¶mping (2019).\n",
    "\n",
    "The notebook is meant to enable the user to re-produce our results, either by fully repeating all calculations, or by loading the results of the time-consuming tasks from the `/data` folder and only creating the graphs. Therefore, the notebook is split into four sections:\n",
    "- Setup: This section imports the required modules and initializes the `shap` package for visual output.\n",
    "\n",
    "- Preparation: This section prepares the dataset (since `shap` cannot work with string-formatted categorical features), trains the base model and computes the SHAP values. It also stores all generated data in a folder (the default is `/data`), and can therefore be skipped when executing the notebook.\n",
    "\n",
    "- Calculation of explanation scores: This is the calculation of minimal full-explanation coalitions with ICECREAM. The `/original_data` folder already contains the original results of this calculation, so it can also be skipped.\n",
    "\n",
    "- Analysis: This section loads the data from the previous sections and performs the same analysis we are showing in the paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import set_config\n",
    "import shap\n",
    "\n",
    "from explain.explanations.model_explanation import find_minimum_size_coalitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we prepare the data, train a random forest classifier and calculate the SHAP values of this classifier for all samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folder for data storage and loading (pre-calculated data is stored at `/original`)\n",
    "folder = 'data'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "credit_data = pd.read_csv(f'german_credit_data.csv')\n",
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "data = credit_data.copy()\n",
    "model_target = 'credit_risk'\n",
    "\n",
    "numerical_features = data.drop(model_target, axis=1).select_dtypes(include='int64').columns.tolist()\n",
    "categorical_features = data.drop(model_target, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "model_features = categorical_features + numerical_features\n",
    "\n",
    "# Convert categorical values to the integer codes so that SHAP can handle them\n",
    "data[categorical_features] = data[categorical_features].astype('category').apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store pre-processed credit data\n",
    "with open(f'{folder}/data.pkl', 'wb') as f:\n",
    "  pickle.dump(data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train classifier\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, shuffle=True, random_state=23)\n",
    "\n",
    "X_train = train_data.drop(model_target, axis=1)\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "X_test = test_data.drop(model_target, axis=1)\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "X_total = data.drop(model_target, axis=1)\n",
    "y_total = data[model_target]\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [\n",
    "        (\"num_imputer\", SimpleImputer(strategy=\"mean\", add_indicator=True)),\n",
    "        (\"num_scaler\", MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline(\n",
    "    [\n",
    "        (\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine all data preprocessors from above\n",
    "data_processor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_processing\", numerical_processor, numerical_features),\n",
    "        (\"categorical_processing\", categorical_processor, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "random_forest = RandomForestClassifier(max_depth=10, random_state=1)\n",
    "\n",
    "# Pipeline with desired data transformers, along with an estimator at the end\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_processing\", data_processor),\n",
    "        (\"rf\", random_forest),\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifier = pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(f'Accuracy (train / test): {accuracy_score(y_train, classifier.predict(X_train)):.2f} / {accuracy_score(y_test, classifier.predict(X_test)):.2f}')\n",
    "\n",
    "# Show pipeline\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store classifier\n",
    "with open(f'{folder}/classifier.pkl', 'wb') as f:\n",
    "  pickle.dump(classifier, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "explainer = shap.Explainer(classifier.predict, X_total)\n",
    "shap_values = explainer(X_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show SHAP waterfall for some sample\n",
    "shap.plots.waterfall(shap_values[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store SHAP values\n",
    "with open(f'{folder}/shap.pkl', 'wb') as f:\n",
    "  pickle.dump(shap_values, f, protocol=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of explanation scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply ICECREAM to the dataset, calculating minimum-size coalitions with (approximately) full explanation score and at most four elements for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9998\n",
    "k = 4\n",
    "\n",
    "coalitions = [find_minimum_size_coalitions(data, observation, 'credit_risk', classifier=classifier.predict, threshold=alpha, maximum_coalition_size=k) for _, observation in tqdm(list(data.head(50).iterrows()))]\n",
    "\n",
    "with open(f'{folder}/coalitions.pkl', 'wb') as f:\n",
    "    pickle.dump(coalitions, f, protocol=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the SHAP and ICECREAM results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, classifier, SHAP values and minimum-size coalitions from /data\n",
    "\n",
    "with open(f'{folder}/data.pkl', 'rb') as f:\n",
    "  data = pickle.load(f)\n",
    "\n",
    "model_target = 'credit_risk'\n",
    "\n",
    "X_total = data.drop(model_target, axis=1)\n",
    "y_total = data[model_target]\n",
    "\n",
    "model_features = list(X_total.columns)\n",
    "\n",
    "\n",
    "with open(f'{folder}/classifier.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}/shap.pkl', 'rb') as f:\n",
    "    shap_values = pickle.load(f)\n",
    "    shap_df = pd.DataFrame(shap_values.values, columns=model_features)\n",
    "    normalized_shap_df = shap_df.div(shap_df.sum(axis=1), axis=0)\n",
    "\n",
    "with open(f'{folder}/coalitions.pkl', 'rb') as f:\n",
    "    minimum_size_coalitions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify simple samples, i.e., samples which have at least one few-feature, full-explanation score coalition\n",
    "simple_explanation_indices = [i for i, coalitions in enumerate(minimum_size_coalitions) if len(coalitions) > 0]\n",
    "\n",
    "print(f'There are {len(simple_explanation_indices)}/{len(minimum_size_coalitions)} samples with simple (k <= 4, alpha >= 0.9998) explanations: {simple_explanation_indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all such coalitions, together with the sample they refer to\n",
    "coalitions = pd.DataFrame([(index, coalition) for index in simple_explanation_indices for coalition in minimum_size_coalitions[index]], columns=['sample_index', 'coalition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successively randomize features in three different orders and compare the prediction stability of the model\n",
    "\n",
    "def create_randomized_samples(features, number_of_samples=1_000):\n",
    "    return X_total[features].sample(n=number_of_samples, replace=True, ignore_index=True)\n",
    "\n",
    "number_of_samples = 1_000\n",
    "\n",
    "result_non_coalition_features, result_non_top_shap_features, result_random_features = [], [], []\n",
    "for index, [sample_index, coalition] in tqdm(coalitions.iterrows(), total=len(coalitions)):\n",
    "    non_top_shap_features = list(normalized_shap_df.iloc[sample_index].sort_values(ascending=True).index)\n",
    "    non_coalition_features = sorted(non_top_shap_features, key=lambda feature: feature in coalition)\n",
    "    random_features = model_features.copy()\n",
    "    random.shuffle(random_features)\n",
    "\n",
    "    congruence_non_coalition_features, congruence_non_top_shap_features, congruence_random_features = [], [], []\n",
    "    for number_of_randomized_features in range(len(model_features) + 1):\n",
    "        # Randomize in order of non_coalition_features and measure congruence\n",
    "        samples = X_total.loc[[sample_index] * number_of_samples].reset_index(drop=True)\n",
    "        samples[non_coalition_features[:number_of_randomized_features]] = create_randomized_samples(non_coalition_features[:number_of_randomized_features], number_of_samples=number_of_samples)\n",
    "\n",
    "        congruence_non_coalition_features.append((classifier.predict(samples) == y_total[sample_index]).mean())\n",
    "\n",
    "        # Randomize in order of non_top_shap_features and measure congruence\n",
    "        samples = X_total.loc[[sample_index] * number_of_samples].reset_index(drop=True)\n",
    "        samples[non_top_shap_features[:number_of_randomized_features]] = create_randomized_samples(non_top_shap_features[:number_of_randomized_features], number_of_samples=number_of_samples)\n",
    "\n",
    "        congruence_non_top_shap_features.append((classifier.predict(samples) == y_total[sample_index]).mean())\n",
    "\n",
    "        # Randomize in order of random_features and measure congruence\n",
    "        samples = X_total.loc[[sample_index] * number_of_samples].reset_index(drop=True)\n",
    "        samples[random_features[:number_of_randomized_features]] = create_randomized_samples(random_features[:number_of_randomized_features], number_of_samples=number_of_samples)\n",
    "\n",
    "        congruence_random_features.append((classifier.predict(samples) == y_total[sample_index]).mean())\n",
    "\n",
    "    result_non_top_shap_features.append(congruence_non_top_shap_features)\n",
    "    result_non_coalition_features.append(congruence_non_coalition_features)\n",
    "    result_random_features.append(congruence_random_features)\n",
    "\n",
    "y_non_top_shap_features = np.array(result_non_top_shap_features)\n",
    "y_non_coalition_features = np.array(result_non_coalition_features)\n",
    "y_random_features = np.array(result_random_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustration_sample_index = 680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "colors=['#009E73', '#E69F00', '#0072B2']\n",
    "\n",
    "# Plot illustration sample\n",
    "plt.plot(y_non_coalition_features[illustration_sample_index], '.-', label='Randomize non-coalition features first', color=colors[0])\n",
    "plt.plot(y_non_top_shap_features[illustration_sample_index], 'x-', label='Randomize in ascending order of SHAP value', color=colors[1])\n",
    "plt.plot(y_random_features[illustration_sample_index], 's-', label='Randomize in random order', color=colors[2], markersize=3)\n",
    "\n",
    "# Plot mean and error bars for all samples\n",
    "plt.plot(np.mean(y_non_coalition_features, axis=0), '--', color=colors[0])\n",
    "plt.fill_between(range(len(model_features) + 1), np.quantile(y_non_coalition_features, 0.05, axis=0), np.quantile(y_non_coalition_features, 0.95, axis=0), color=colors[0], alpha=0.2)\n",
    "plt.plot(np.mean(y_non_top_shap_features, axis=0), '--', color=colors[1])\n",
    "plt.fill_between(range(len(model_features) + 1), np.quantile(y_non_top_shap_features, 0.05, axis=0), np.quantile(y_non_top_shap_features, 0.95, axis=0), color=colors[1], alpha=0.2)\n",
    "plt.plot(np.mean(y_random_features, axis=0), '--', color=colors[2])\n",
    "plt.fill_between(range(len(model_features) + 1), np.quantile(y_random_features, 0.05, axis=0), np.quantile(y_random_features, 0.95, axis=0), color=colors[2], alpha=0.2)\n",
    "\n",
    "# Show where coalition features are randomized\n",
    "plt.annotate('Start randomizing \\n coalition features', xy=(18.2, 0.998), xytext=(20, 0.98), backgroundcolor='w', arrowprops=dict(arrowstyle='-|>', facecolor='gray', edgecolor='gray'))\n",
    "\n",
    "plt.ylabel(r'Stability $\\mathbb{P}[Y=y \\mid do(\\mathbf{V}_C = \\mathbf{v}_C)]$ of prediction')\n",
    "plt.xlabel('Number of randomized features')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'{folder}/result.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
